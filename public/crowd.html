<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Base de Donn√©es Avanc√©e d'Extraction de Donn√©es de Foule</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
            min-height: 100vh;
        }

        .container {
            max-width: 1800px;
            margin: 0 auto;
            padding: 20px;
        }

        .header {
            text-align: center;
            margin-bottom: 30px;
            color: white;
        }

        .header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .header p {
            font-size: 1.2em;
            opacity: 0.9;
            max-width: 800px;
            margin: 0 auto;
        }

        .controls {
            background: rgba(255,255,255,0.95);
            padding: 30px;
            border-radius: 15px;
            margin-bottom: 25px;
            box-shadow: 0 8px 32px rgba(0,0,0,0.1);
            backdrop-filter: blur(10px);
        }

        .search-section {
            display: grid;
            grid-template-columns: 1fr 200px;
            gap: 15px;
            margin-bottom: 25px;
        }

        .search-box {
            padding: 15px 20px;
            border: 2px solid #ddd;
            border-radius: 10px;
            font-size: 16px;
            transition: border-color 0.3s;
        }

        .search-box:focus {
            outline: none;
            border-color: #667eea;
        }

        .clear-btn {
            background: #ff6b6b;
            color: white;
            border: none;
            padding: 15px 25px;
            border-radius: 10px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            transition: background 0.3s;
        }

        .clear-btn:hover {
            background: #ff5252;
        }

        .filters {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
        }

        .filter-group {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 12px;
            border: 1px solid #e9ecef;
        }

        .filter-group h3 {
            margin-bottom: 15px;
            color: #495057;
            font-size: 15px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .filter-options {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }

        .filter-tag {
            background: #e9ecef;
            color: #495057;
            padding: 8px 16px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 13px;
            transition: all 0.3s;
            border: 2px solid transparent;
            font-weight: 500;
        }

        .filter-tag:hover {
            background: #dee2e6;
            transform: translateY(-1px);
        }

        .filter-tag.active {
            background: #667eea;
            color: white;
            border-color: #5a6fd8;
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
        }

        .stats {
            text-align: center;
            margin: 20px 0;
            color: #666;
            font-size: 16px;
            font-weight: 500;
        }

        .methods-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(600px, 1fr));
            gap: 25px;
        }

        .method-card {
            background: rgba(255,255,255,0.98);
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 6px 30px rgba(0,0,0,0.1);
            transition: transform 0.3s, box-shadow 0.3s;
            border: 1px solid rgba(255,255,255,0.8);
        }

        .method-card:hover {
            transform: translateY(-8px);
            box-shadow: 0 12px 40px rgba(0,0,0,0.15);
        }

        .method-title {
            font-size: 1.5em;
            font-weight: bold;
            margin-bottom: 15px;
            color: #2c3e50;
            line-height: 1.3;
        }

        .method-description {
            color: #666;
            margin-bottom: 25px;
            line-height: 1.7;
            font-size: 15px;
        }

        .method-details {
            margin-bottom: 25px;
        }

        .detail-section {
            margin-bottom: 20px;
            border-left: 4px solid #667eea;
            padding-left: 15px;
            background: rgba(102, 126, 234, 0.02);
            padding: 15px;
            border-radius: 8px;
        }

        .detail-title {
            font-weight: 700;
            color: #2c3e50;
            font-size: 14px;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.8px;
        }

        .detail-content {
            color: #555;
            font-size: 14px;
            line-height: 1.6;
        }

        .detail-list {
            list-style: none;
            padding-left: 0;
        }

        .detail-list li {
            margin-bottom: 8px;
            padding-left: 20px;
            position: relative;
        }

        .detail-list li::before {
            content: "‚ñ∂";
            color: #667eea;
            font-weight: bold;
            position: absolute;
            left: 0;
            font-size: 12px;
        }

        .scenario-section {
            background: #e8f4f8;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 15px;
            border-left: 4px solid #17a2b8;
        }

        .scenario-title {
            font-weight: 600;
            color: #0c5460;
            margin-bottom: 8px;
            font-size: 13px;
        }

        .scenario-content {
            color: #0c5460;
            font-size: 13px;
            line-height: 1.5;
        }

        .implementation-section {
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 15px;
            border-left: 4px solid #ffc107;
        }

        .implementation-title {
            font-weight: 600;
            color: #856404;
            margin-bottom: 8px;
            font-size: 13px;
        }

        .implementation-content {
            color: #856404;
            font-size: 13px;
            line-height: 1.5;
        }

        .references {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 10px;
            margin-top: 20px;
            border: 1px solid #e9ecef;
        }

        .references h4 {
            font-size: 13px;
            margin-bottom: 12px;
            color: #495057;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            font-weight: 600;
        }

        .reference-link {
            display: block;
            color: #667eea;
            text-decoration: none;
            font-size: 12px;
            margin-bottom: 6px;
            word-break: break-all;
            padding: 4px 8px;
            border-radius: 4px;
            transition: background 0.2s;
        }

        .reference-link:hover {
            background: rgba(102, 126, 234, 0.1);
            text-decoration: underline;
        }

        .tags-container {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-top: 20px;
        }

        .tag {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 6px 12px;
            border-radius: 20px;
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.3px;
        }

        .privacy-critique { background: linear-gradient(135deg, #dc3545, #c82333); }
        .privacy-elevee { background: linear-gradient(135deg, #fd7e14, #e55a00); }
        .privacy-moyenne { background: linear-gradient(135deg, #ffc107, #e0a800); }
        .privacy-faible { background: linear-gradient(135deg, #28a745, #1e7e34); }

        .participation-dedique { background: linear-gradient(135deg, #007bff, #0056b3); }
        .participation-actif { background: linear-gradient(135deg, #6f42c1, #5a2d91); }
        .participation-minimal { background: linear-gradient(135deg, #6c757d, #545b62); }
        .participation-aucune { background: linear-gradient(135deg, #343a40, #23272b); }

        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            margin: 10px 0;
            overflow-x: auto;
        }

        @media (max-width: 768px) {
            .search-section {
                grid-template-columns: 1fr;
            }

            .filters {
                grid-template-columns: 1fr;
            }

            .methods-grid {
                grid-template-columns: 1fr;
            }

            .method-card {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üé≠ Base de Donn√©es Avanc√©e d'Extraction de Donn√©es de Foule</h1>
            <p>Collection compl√®te de techniques pour extraire des donn√©es comportementales, physiologiques et sociales des foules avec impl√©mentations d√©taill√©es et sc√©narios d'usage</p>
        </div>

        <div class="controls">
            <div class="search-section">
                <input type="text" id="searchInput" class="search-box" placeholder="Rechercher par m√©thode, technologie, mat√©riel, ou application...">
                <button onclick="clearFilters()" class="clear-btn">Effacer Tout</button>
            </div>

            <div class="filters">
                <div class="filter-group">
                    <h3>Type de Donn√©es</h3>
                    <div class="filter-options" id="dataTypeFilter">
                        <span class="filter-tag" data-filter="comportemental">Comportemental</span>
                        <span class="filter-tag" data-filter="physiologique">Physiologique</span>
                        <span class="filter-tag" data-filter="spatial">Spatial</span>
                        <span class="filter-tag" data-filter="acoustique">Acoustique</span>
                        <span class="filter-tag" data-filter="biometrique">Biom√©trique</span>
                        <span class="filter-tag" data-filter="electromagnetique">√âlectromagn√©tique</span>
                        <span class="filter-tag" data-filter="environnemental">Environnemental</span>
                        <span class="filter-tag" data-filter="social">Social</span>
                    </div>
                </div>

                <div class="filter-group">
                    <h3>Niveau de Confidentialit√©</h3>
                    <div class="filter-options" id="privacyFilter">
                        <span class="filter-tag" data-filter="faible">Faible</span>
                        <span class="filter-tag" data-filter="moyenne">Moyenne</span>
                        <span class="filter-tag" data-filter="elevee">√âlev√©e</span>
                        <span class="filter-tag" data-filter="critique">Critique</span>
                    </div>
                </div>

                <div class="filter-group">
                    <h3>Participation Requise</h3>
                    <div class="filter-options" id="participationFilter">
                        <span class="filter-tag" data-filter="aucune">Aucune</span>
                        <span class="filter-tag" data-filter="minimal">Minimale</span>
                        <span class="filter-tag" data-filter="actif">Active</span>
                        <span class="filter-tag" data-filter="dedique">D√©di√©e</span>
                    </div>
                </div>

                <div class="filter-group">
                    <h3>Type de Technologie</h3>
                    <div class="filter-options" id="technologyFilter">
                        <span class="filter-tag" data-filter="capteurs">Capteurs</span>
                        <span class="filter-tag" data-filter="vision-par-ordinateur">Vision par Ordinateur</span>
                        <span class="filter-tag" data-filter="signaux-rf">Signaux RF</span>
                        <span class="filter-tag" data-filter="appareils-mobiles">Appareils Mobiles</span>
                        <span class="filter-tag" data-filter="infrastructure-serveur">Infrastructure Serveur</span>
                        <span class="filter-tag" data-filter="ia-ml">IA/ML</span>
                        <span class="filter-tag" data-filter="raspberry-pi">Raspberry Pi</span>
                    </div>
                </div>

                <div class="filter-group">
                    <h3>Impl√©mentation</h3>
                    <div class="filter-options" id="implementationFilter">
                        <span class="filter-tag" data-filter="passif">Passif</span>
                        <span class="filter-tag" data-filter="actif">Actif</span>
                        <span class="filter-tag" data-filter="interactif">Interactif</span>
                        <span class="filter-tag" data-filter="infrastructure">Infrastructure</span>
                    </div>
                </div>

                <div class="filter-group">
                    <h3>Applications Spectacle</h3>
                    <div class="filter-options" id="showFilter">
                        <span class="filter-tag" data-filter="temps-reel">Temps R√©el</span>
                        <span class="filter-tag" data-filter="interactif-public">Interactif Public</span>
                        <span class="filter-tag" data-filter="adaptatif">Adaptatif</span>
                        <span class="filter-tag" data-filter="visualisation">Visualisation</span>
                    </div>
                </div>
            </div>

            <div class="stats" id="statsDisplay">
                Affichage de <span id="visibleCount">0</span> sur <span id="totalCount">0</span> m√©thodes
            </div>
        </div>

        <div class="methods-grid" id="methodsGrid">
            <!-- Methods will be populated by JavaScript -->
        </div>
    </div>

    <script>
        // D√©but du JavaScript - sera compl√©t√© dans les parties suivantes
        // PARTIE 2 - Donn√©es des m√©thodes
        const methods = [
            {
                title: "Analyse des Vibrations Pi√©zo√©lectriques du Sol",
                description: "Installation de capteurs pi√©zo√©lectriques sous le plancher pour d√©tecter et analyser les mouvements de foule, les pas, les sauts et les comportements rythmiques collectifs √† travers les patterns de vibrations structurelles.",
                dataTypes: ["comportemental", "spatial"],
                privacy: "faible",
                participation: "aucune",
                implementation: "infrastructure",
                technology: "capteurs",
                showApplications: ["temps-reel", "visualisation", "adaptatif"],
                hardware: [
                    "Acc√©l√©rom√®tres PCB Piezotronics 393A03 (plage ¬±50g)",
                    "G√©ophones Geospace GS-20DX (sensibilit√© 28.8V/m/s)",
                    "Acquisition de donn√©es National Instruments cDAQ-9178 (250kS/s)",
                    "Montage structurel sur dalles b√©ton/poutres acier"
                ],
                scenarios: [
                    "Concert : D√©tection automatique des moments de danse collective, mesure de l'intensit√© rythmique du public synchronis√©e avec la musique",
                    "Festival : Cartographie en temps r√©el des zones d'activit√©, d√©tection des mouvements de foule entre sc√®nes",
                    "Th√©√¢tre : Mesure des r√©actions physiques du public (applaudissements, mouvements de surprise)",
                    "Spectacle interactif : Contr√¥le de l'√©clairage et des effets visuels bas√© sur l'√©nergie collective mesur√©e"
                ],
                implementations: [
                    "Raspberry Pi avec HAT d'acquisition analogique pour traitement local",
                    "Filtrage num√©rique en temps r√©el pour s√©parer pas humains du bruit ambiant",
                    "Algorithmes de d√©tection d'√©v√©nements (saut, applaudissement, danse)",
                    "Interface OSC/MIDI pour contr√¥le de logiciels de spectacle (Ableton Live, TouchDesigner)"
                ],
                interactiveShows: [
                    "Mur LED r√©actif √† l'intensit√© de danse du public",
                    "Syst√®me sonore adaptatif qui ajuste le volume selon l'engagement corporel",
                    "Effets pyrotechniques d√©clench√©s par des pics d'activit√© collective",
                    "Composition musicale g√©n√©rative bas√©e sur les rythmes d√©tect√©s du public"
                ],
                technicalSpecs: [
                    "Plage de fr√©quence : 0.1-1000Hz avec traitement FFT",
                    "R√©solution spatiale : 2-5m selon densit√© des capteurs",
                    "R√©solution temporelle : Timestamps de 1ms",
                    "D√©tection multi-modale : marche, course, saut, danse"
                ],
                limitations: [
                    "Interf√©rences des syst√®mes HVAC et vibrations externes",
                    "Att√©nuation du signal avec la distance de la source",
                    "N√©cessite installation permanente dans structure du b√¢timent"
                ],
                references: [
                    "https://doi.org/10.1016/j.engstruct.2018.12.084",
                    "https://www.pcb.com/products?model=393a03",
                    "https://geospace.com/products/geophones/gs-20dx/",
                    "https://www.ni.com/en-us/support/model.cdaq-9178.html"
                ]
            },
            {
                title: "Analyse Collective du Mouvement par Gyroscope Mobile",
                description: "Exploitation des gyroscopes int√©gr√©s dans les smartphones pour d√©tecter les balancements collectifs, rotations et mouvements synchronis√©s lors d'√©v√©nements, fournissant une d√©tection de mouvement distribu√©e √† travers la foule.",
                dataTypes: ["comportemental", "physiologique"],
                privacy: "moyenne",
                participation: "actif",
                implementation: "interactif",
                technology: "appareils-mobiles",
                showApplications: ["temps-reel", "interactif-public", "adaptatif"],
                hardware: [
                    "Gyroscopes MEMS int√©gr√©s (plage ¬±2000¬∞/s)",
                    "Acc√©l√©rom√®tres pour compensation de mouvement",
                    "WebRTC pour streaming de donn√©es temps r√©el",
                    "Edge computing pour traitement local"
                ],
                scenarios: [
                    "Concert symphonique : D√©tection du balancement collectif du public pendant les passages lyriques",
                    "Match sportif : Mesure de l'intensit√© des c√©l√©brations et de la synchronisation des supporteurs",
                    "Rave party : Analyse des patterns de danse collectifs et cr√©ation de visualisations synchronis√©es",
                    "M√©ditation collective : Monitoring des mouvements de respiration group√©e"
                ],
                implementations: [
                    "Application web progressive avec API DeviceOrientationEvent",
                    "WebSocket server pour agr√©gation temps r√©el des donn√©es gyroscopiques",
                    "Algorithmes de d√©tection de synchronisation bas√©s sur corr√©lation crois√©e",
                    "Interface de visualisation 3D des mouvements collectifs"
                ],
                interactiveShows: [
                    "√âclairage qui suit les mouvements collectifs du public en temps r√©el",
                    "Synth√®se sonore modul√©e par l'intensit√© et la direction des mouvements",
                    "Projection mapping r√©agissant aux oscillations et rotations d√©tect√©es",
                    "Jeu interactif o√π le public contr√¥le des √©l√©ments visuels par mouvement coordonn√©"
                ],
                technicalSpecs: [
                    "Fr√©quence d'√©chantillonnage : 60Hz typique, jusqu'√† 200Hz possible",
                    "Pr√©cision v√©locit√© angulaire : 0.1¬∞/s",
                    "Latence : 50-200ms selon r√©seau",
                    "Consommation batterie : 15-30% par heure"
                ],
                limitations: [
                    "N√©cessite participation active et installation d'application",
                    "L'orientation de l'appareil affecte la pr√©cision",
                    "La latence r√©seau impacte la synchronisation temps r√©el"
                ],
                references: [
                    "https://developer.mozilla.org/en-US/docs/Web/API/DeviceOrientationEvent",
                    "https://doi.org/10.1145/3210240.3210334",
                    "https://developers.google.com/web/fundamentals/native-hardware/device-orientation",
                    "https://caniuse.com/deviceorientation"
                ]
            },
            {
                title: "Serveur de Triangulation WiFi D√©di√©",
                description: "Configuration de multiples points d'acc√®s WiFi avec firmware personnalis√© pour effectuer une triangulation pr√©cise des positions d'appareils bas√©e sur les mesures de force du signal et calculs de temps de vol.",
                dataTypes: ["spatial", "comportemental"],
                privacy: "elevee",
                participation: "aucune",
                implementation: "infrastructure",
                technology: "infrastructure-serveur",
                showApplications: ["temps-reel", "visualisation"],
                hardware: [
                    "Routeurs compatibles OpenWrt (TP-Link Archer C7, Netgear R7800)",
                    "Antennes directionnelles haute gain (8-15dBi)",
                    "Infrastructure serveur d√©di√©e (Intel NUC, cluster Raspberry Pi)",
                    "Synchronisation r√©seau via NTP/PTP"
                ],
                scenarios: [
                    "Festival multi-sc√®nes : Tracking anonyme des flux de public entre diff√©rentes attractions",
                    "Mus√©e interactif : Analyse des parcours de visite et temps d'arr√™t devant les ≈ìuvres",
                    "Conf√©rence : Cartographie des zones de networking et interactions sociales",
                    "Club nocturne : Optimisation de l'agencement bas√©e sur les patterns de circulation"
                ],
                implementations: [
                    "Cluster Raspberry Pi 4 avec antennes WiFi externes pour triangulation",
                    "Base de donn√©es time-series (InfluxDB) pour stockage des positions",
                    "Algorithmes de trilat√©ration avec filtres de Kalman pour trajectoires fluides",
                    "Dashboard temps r√©el avec cartes de chaleur de densit√©"
                ],
                interactiveShows: [
                    "√âclairage architectural qui s'adapte aux zones de forte densit√©",
                    "Diffusion audio localis√©e suivant les mouvements du public",
                    "R√©v√©lation progressive d'≈ìuvres selon les patterns d'exploration",
                    "Interactions sonores d√©clench√©es par la proximit√© entre participants"
                ],
                technicalSpecs: [
                    "Pr√©cision triangulation : 1-5m selon densit√© des AP",
                    "Fr√©quence de mise √† jour : 1-10Hz par appareil",
                    "Tracking simultan√© : 500+ appareils",
                    "Zone de couverture : 100-500m¬≤ par AP"
                ],
                limitations: [
                    "Les appareils modernes utilisent la randomisation d'adresse MAC",
                    "Interf√©rences signal en environnements RF denses",
                    "N√©cessite investissement infrastructure substantiel"
                ],
                references: [
                    "https://openwrt.org/",
                    "https://doi.org/10.1109/TMC.2016.2632715",
                    "https://github.com/openwrt/openwrt",
                    "https://www.kernel.org/doc/html/latest/networking/mac80211-injection.html"
                ]
            },
            {
                title: "Estimation de Pose Multi-Cam√©ras par Vision par Ordinateur",
                description: "R√©seau de cam√©ras synchronis√©es utilisant l'estimation de pose aliment√©e par IA pour tracker les positions corporelles individuelles, gestes et patterns de mouvement collectifs en temps r√©el.",
                dataTypes: ["comportemental", "biometrique"],
                privacy: "critique",
                participation: "aucune",
                implementation: "passif",
                technology: "vision-par-ordinateur",
                showApplications: ["temps-reel", "adaptatif", "visualisation"],
                hardware: [
                    "Cam√©ras IP 4K avec objectifs varifocaux (2.8-12mm)",
                    "NVIDIA Jetson AGX Orin pour traitement edge",
                    "Enregistreurs vid√©o r√©seau avec connectivit√© 10GbE",
                    "Clusters GPU pour estimation de pose temps r√©el"
                ],
                scenarios: [
                    "Performance de danse : Analyse des patterns chor√©graphiques et synchronisation des danseurs",
                    "Concert classique : Mesure de l'engagement corporel du public (posture, applaudissements)",
                    "Th√©√¢tre immersif : Adaptation de la performance selon les r√©actions gestuelles du public",
                    "Installation artistique : Cr√©ation de visualisations bas√©es sur les poses et mouvements collectifs"
                ],
                implementations: [
                    "OpenPose ou MediaPipe pour d√©tection de points-cl√©s corporels",
                    "Raspberry Pi clusters avec Coral USB Accelerator pour traitement distribu√©",
                    "Fusion multi-cam√©ras pour reconstruction 3D des poses",
                    "Anonymisation temps r√©el par floutage automatique des visages"
                ],
                interactiveShows: [
                    "Projection mapping r√©active aux gestes et postures du public",
                    "Environnement sonore spatial modul√© par les mouvements corporels",
                    "√âclairage qui accentue et amplifie les gestuelles collectives",
                    "Avatar num√©rique collectif repr√©sentant l'√©nergie gestuelle de la foule"
                ],
                technicalSpecs: [
                    "Points-cl√©s de pose : 17-133 landmarks corporels par personne",
                    "Vitesse de traitement : 30fps pour 10+ personnes simultan√©ment",
                    "Pr√©cision : ¬±5cm pour articulations cl√©s du corps",
                    "Port√©e : 5-50m selon configuration cam√©ras"
                ],
                limitations: [
                    "Implications de confidentialit√© s√©v√®res n√©cessitant consentement",
                    "Performance d√©grad√©e dans foules tr√®s denses",
                    "Exigences computationnelles √©lev√©es pour traitement temps r√©el"
                ],
                references: [
                    "https://github.com/CMU-Perceptual-Computing-Lab/openpose",
                    "https://mediapipe.dev/solutions/pose",
                    "https://doi.org/10.1109/TPAMI.2019.2929257",
                    "https://developer.nvidia.com/embedded/jetson-agx-orin"
                ]
            },
            {
                title: "Surveillance Thermique de Densit√© et Temp√©rature de Foule",
                description: "D√©ploiement de cam√©ras thermiques pour surveiller la densit√© de foule, d√©tecter la fi√®vre/temp√©ratures √©lev√©es, et analyser les patterns de chaleur pour d√©tection de stress et gestion de foule.",
                dataTypes: ["physiologique", "spatial", "environnemental"],
                privacy: "moyenne",
                participation: "aucune",
                implementation: "passif",
                technology: "capteurs",
                showApplications: ["temps-reel", "visualisation"],
                hardware: [
                    "Cam√©ras thermiques FLIR A655sc (r√©solution 640√ó480)",
                    "Optris PI 1M avec r√©solution thermique 382√ó288",
                    "Sources de corps noir pour calibration thermique",
                    "Capteurs de compensation environnementale"
                ],
                scenarios: [
                    "Festival d'√©t√© : Monitoring de la temp√©rature corporelle collective pour pr√©vention des coups de chaleur",
                    "Salle de concert : Visualisation de l'√©chauffement √©motionnel pendant les climax musicaux",
                    "Convention : D√©tection automatique des zones de surcharge thermique",
                    "Performance sportive : Analyse de l'intensit√© physique des spectateurs"
                ],
                implementations: [
                    "Raspberry Pi avec cam√©ra thermique MLX90640 pour monitoring low-cost",
                    "Traitement d'image thermique avec OpenCV et gradient mapping",
                    "D√©tection automatique de fi√®vre avec seuils adaptatifs",
                    "Visualisation temps r√©el des cartes de chaleur sur √©crans publics"
                ],
                interactiveShows: [
                    "Visualisation artistique des 'auras' thermiques du public en temps r√©el",
                    "Modulation de la temp√©rature ambiante bas√©e sur l'√©chauffement collectif",
                    "√âclairage color√© repr√©sentant les variations thermiques de la foule",
                    "Installation interactive o√π les zones chaudes d√©clenchent des effets visuels"
                ],
                technicalSpecs: [
                    "Pr√©cision temp√©rature : ¬±0.02¬∞C avec calibration",
                    "Sensibilit√© thermique : <0.02¬∞C NETD",
                    "Fr√©quence d'images : 50Hz pleine r√©solution",
                    "Plage op√©rationnelle : -20¬∞C √† +150¬∞C"
                ],
                limitations: [
                    "Affect√© par temp√©rature ambiante et humidit√©",
                    "Port√©e limit√©e pour mesure pr√©cise de temp√©rature",
                    "Ne peut identifier les individus, seulement signatures thermiques"
                ],
                references: [
                    "https://www.flir.com/products/a655sc/",
                    "https://www.optris.com/thermal-imager-pi-1m",
                    "https://doi.org/10.1016/j.buildenv.2020.107025",
                    "https://ieeexplore.ieee.org/document/9001197"
                ]
            },
            {
                title: "R√©seau Maill√© de Balises Bluetooth Low Energy",
                description: "D√©ploiement dense de balises BLE cr√©ant un r√©seau maill√© pour d√©tection de proximit√© pr√©cise, tra√ßage de contacts et conscience de micro-localisation √† travers le lieu.",
                dataTypes: ["spatial", "comportemental"],
                privacy: "moyenne",
                participation: "minimal",
                implementation: "infrastructure",
                technology: "signaux-rf",
                showApplications: ["temps-reel", "interactif-public"],
                hardware: [
                    "Balises Nordic nRF52840 SoC",
                    "Balises de localisation Estimote avec batterie 5+ ans",
                    "N≈ìuds passerelle Raspberry Pi",
                    "R√©seau maill√© Bluetooth 5.0+"
                ],
                scenarios: [
                    "Exposition interactive : D√©clenchement de contenu personnalis√© selon proximit√© des ≈ìuvres",
                    "Concert en plein air : Navigation indoor et livraison de contenu bas√©e sur position",
                    "Festival gastronomique : Recommandations de stands selon parcours et pr√©f√©rences",
                    "Th√©√¢tre immersif : Adaptation de l'exp√©rience selon position du spectateur"
                ],
                implementations: [
                    "Raspberry Pi Zero W comme r√©cepteurs BLE avec antennes externes",
                    "Protocole de mesh networking pour topologie auto-r√©paratrice",
                    "Base de donn√©es de proximit√© temps r√©el avec Redis",
                    "Application mobile avec SDK de d√©tection de proximit√©"
                ],
                interactiveShows: [
                    "Audio spatialis√© qui suit le public dans l'espace",
                    "√âclairage personnel qui accompagne chaque spectateur",
                    "D√©clenchement d'effets interactifs par proximit√© entre participants",
                    "Narration adaptative selon le parcours choisi par le public"
                ],
                technicalSpecs: [
                    "Port√©e : 1-70m puissance de transmission ajustable",
                    "Autonomie batterie : 2-5 ans selon configuration",
                    "Pr√©cision : 1-3m d√©tection de proximit√©",
                    "R√©seau maill√© : Topologie auto-r√©paratrice"
                ],
                limitations: [
                    "N√©cessite Bluetooth activ√© sur appareil utilisateur",
                    "Absorption signal par corps humains affecte pr√©cision",
                    "Pr√©occupations de confidentialit√© avec tracking persistant d'appareils"
                ],
                references: [
                    "https://www.nordicsemi.com/Products/nRF52840",
                    "https://estimote.com/products/",
                    "https://www.bluetooth.com/specifications/mesh-specifications/",
                    "https://doi.org/10.1109/JIOT.2019.2918240"
                ]
            },
            {
                title: "Triangulation Acoustique avec R√©seaux de Microphones",
                description: "R√©seaux de microphones distribu√©s utilisant la triangulation acoustique pour localiser les sources sonores, tracker les voix individuelles et analyser les patterns audio de foule pour inf√©rence comportementale.",
                dataTypes: ["acoustique", "spatial", "comportemental"],
                privacy: "elevee",
                participation: "aucune",
                implementation: "infrastructure",
                technology: "capteurs",
                showApplications: ["temps-reel", "adaptatif"],
                hardware: [
                    "R√©seaux de microphones MEMS (32-64 √©l√©ments)",
                    "Microphones professionnels Audio-Technica AT4050",
                    "Processeurs de beamforming (Texas Instruments C674x DSP)",
                    "Syst√®mes ADC synchronis√©s avec timing GPS"
                ],
                scenarios: [
                    "Concert interactif : Localisation des applaudissements et cris d'encouragement",
                    "D√©bat public : Analyse de l'intensit√© des r√©actions par zone de la salle",
                    "Performance th√©√¢trale : Mesure de l'engagement √©motionnel par analyse vocale collective",
                    "Festival : Cartographie sonore des diff√©rentes ambiances par zone"
                ],
                implementations: [
                    "Raspberry Pi avec cartes d'acquisition audio multi-canaux",
                    "Algorithmes de beamforming en temps r√©el avec GNU Radio",
                    "D√©tection d'√©v√©nements acoustiques (applaudissements, rires, cris)",
                    "Visualisation 3D des sources sonores dans l'espace"
                ],
                interactiveShows: [
                    "Retour audio adaptatif amplifiant les r√©actions positives du public",
                    "Visualisation en temps r√©el de l'√©nergie sonore collective",
                    "Modulation des effets selon l'intensit√© et la localisation des r√©actions",
                    "Composition musicale g√©n√©rative bas√©e sur les patterns acoustiques d√©tect√©s"
                ],
                technicalSpecs: [
                    "Pr√©cision de localisation : ¬±30cm en conditions optimales",
                    "Plage de fr√©quence : 20Hz-20kHz",
                    "Gamme dynamique : 120dB SNR",
                    "Latence de traitement : <10ms temps r√©el"
                ],
                limitations: [
                    "Implications de confidentialit√© s√©v√®res avec enregistrement vocal",
                    "Performance d√©grad√©e avec bruit ambiant √©lev√©",
                    "N√©cessite calibration acoustique complexe"
                ],
                references: [
                    "https://www.ti.com/product/TLV320ADC5140",
                    "https://doi.org/10.1121/1.5042348",
                    "https://github.com/respeaker/mic_array",
                    "https://www.analog.com/en/technical-articles/mems-microphone-array-beamforming.html"
                ]
            },
            {
                title: "Beamforming Audio Multi-Canaux pour Localisation de Locuteurs",
                description: "R√©seaux avanc√©s de microphones utilisant des algorithmes de beamforming pour isoler et tracker des locuteurs individuels dans les foules, analyser les patterns de conversation et d√©tecter les indices vocaux √©motionnels.",
                dataTypes: ["acoustique", "comportemental", "spatial"],
                privacy: "critique",
                participation: "aucune",
                implementation: "infrastructure",
                technology: "capteurs",
                showApplications: ["temps-reel", "adaptatif"],
                hardware: [
                    "R√©seau circulaire 32 √©l√©ments (Eigenmike EM32)",
                    "R√©seaux lin√©aires 8-16 microphones MEMS (SPH0645LM4H)",
                    "Processeurs beamforming temps r√©el (SHARC ADSP-21573)",
                    "Syst√®mes cam√©ra acoustique (CAE Acoustic Camera)",
                    "Syst√®mes ADC haute vitesse (24-bit/192kHz)"
                ],
                scenarios: [
                    "D√©bat politique : Analyse de l'intensit√© des r√©actions par zone g√©ographique",
                    "Concert interactif : Amplification s√©lective des encouragements du public",
                    "Conf√©rence : D√©tection automatique des questions depuis l'audience",
                    "Th√©√¢tre participatif : Int√©gration des r√©actions vocales dans la performance"
                ],
                implementations: [
                    "Raspberry Pi 4 avec carte audio multi-canaux pour arrays simples",
                    "GNU Radio pour traitement de signal et beamforming temps r√©el",
                    "Algorithmes de s√©paration de sources aveugle (ICA, FastICA)",
                    "Classification automatique d'√©v√©nements acoustiques"
                ],
                interactiveShows: [
                    "Spatialisation audio suivant les r√©actions vocales du public",
                    "Amplification s√©lective des zones les plus engag√©es",
                    "Composition musicale int√©grant les rythmes vocaux d√©tect√©s",
                    "Visualisation 3D temps r√©el de l'√©nergie acoustique spatiale"
                ],
                technicalSpecs: [
                    "R√©solution spatiale : ¬±5¬∞ pr√©cision angulaire",
                    "Plage de fr√©quence : 100Hz-8kHz pour analyse vocale",
                    "Traitement temps r√©el : <50ms latence",
                    "Sources simultan√©es : 10+ locuteurs concurrents",
                    "Gamme dynamique : 60dB avec suppression de bruit"
                ],
                limitations: [
                    "Implications de confidentialit√© extr√™mes avec isolation vocale",
                    "Performance d√©grad√©e avec forte r√©verb√©ration",
                    "Complexit√© computationnelle augmente avec taille du r√©seau"
                ],
                references: [
                    "https://mhacoustics.com/products/eigenmike-em32",
                    "https://doi.org/10.1109/TSP.2018.2877403",
                    "https://github.com/BinauralSH/Spherical-Harmonic-Transform",
                    "https://www.analog.com/en/products/adsp-21573.html"
                ]
            },
            {
                title: "Classification de Signatures Acoustiques de Foule",
                description: "Classification bas√©e machine learning des signatures audio de foule pour identifier diff√©rents √©tats comportementaux comme excitation, tension, satisfaction ou d√©tresse sans reconnaissance vocale.",
                dataTypes: ["acoustique", "comportemental"],
                privacy: "moyenne",
                participation: "aucune",
                implementation: "passif",
                technology: "ia-ml",
                showApplications: ["temps-reel", "adaptatif"],
                hardware: [
                    "R√©seaux de microphones MEMS distribu√©s",
                    "Processeurs AI edge (Google Coral TPU, Intel Movidius)",
                    "Acc√©l√©rateurs d'extraction de caract√©ristiques acoustiques",
                    "Analyseurs de spectre temps r√©el (Rohde & Schwarz FSW)"
                ],
                scenarios: [
                    "Match sportif : Classification automatique de l'intensit√© des supporteurs",
                    "Concert : Mesure de l'engagement √©motionnel selon les morceaux",
                    "Festival : D√©tection pr√©coce de situations de stress ou panique",
                    "Spectacle pour enfants : Adaptation du rythme selon niveau d'excitation"
                ],
                implementations: [
                    "Raspberry Pi avec USB Coral Accelerator pour inf√©rence locale",
                    "Extraction de caract√©ristiques MFCC, centro√Øde spectral, taux de passage par z√©ro",
                    "Mod√®les CNN entra√Æn√©s sur datasets de foules annot√©es",
                    "API de classification avec scores de confiance temps r√©el"
                ],
                interactiveShows: [
                    "Adaptation automatique du tempo musical selon √©nergie acoustique",
                    "Modulation de l'intensit√© lumineuse bas√©e sur l'excitation d√©tect√©e",
                    "D√©clenchement d'effets sp√©ciaux lors de pics d'enthousiasme",
                    "Retour visuel amplifiant les moments de communion collective"
                ],
                technicalSpecs: [
                    "Pr√©cision classification : 85-92% pour √©tats de foule",
                    "Extraction caract√©ristiques : MFCC, centro√Øde spectral, ZCR",
                    "Fen√™tre de traitement : Segments audio 1-5 secondes",
                    "Fr√©quence de mise √† jour : 10Hz classification continue",
                    "Robustesse au bruit : SNR jusqu'√† 10dB"
                ],
                limitations: [
                    "Biais des donn√©es d'entra√Ænement vers types d'√©v√©nements sp√©cifiques",
                    "Variations culturelles dans expressions vocales de foule",
                    "Interf√©rences bruit ambiant en lieux ext√©rieurs"
                ],
                references: [
                    "https://doi.org/10.1109/TASLP.2019.2947741",
                    "https://coral.ai/products/dev-board/",
                    "https://librosa.org/doc/main/feature.html",
                    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro"
                ]
            },
            {
                title: "Reconnaissance Gestuelle Collective bas√©e Kinect",
                description: "Multiples capteurs Kinect pour d√©tecter et analyser les gestes collectifs, mouvements synchronis√©s et comportements de groupe √† travers tracking de squelette et estimation de pose.",
                dataTypes: ["comportemental", "spatial", "biometrique"],
                privacy: "elevee",
                participation: "aucune",
                implementation: "passif",
                technology: "vision-par-ordinateur",
                showApplications: ["temps-reel", "interactif-public", "adaptatif"],
                hardware: [
                    "Capteurs Azure Kinect DK (unit√©s multiples)",
                    "Kinect v2 avec d√©tection de profondeur IR",
                    "Clusters GPU pour traitement de squelette temps r√©el",
                    "Syst√®mes de d√©clenchement synchronis√© multi-cam√©ras",
                    "Stockage r√©seau pour donn√©es de nuages de points"
                ],
                scenarios: [
                    "Festival de danse : Analyse des patterns de mouvement chor√©graphique collectifs",
                    "Cours de fitness : Mesure de synchronisation d'exercices de groupe",
                    "Concert pop : Reconnaissance de gestes de foule (salut, applaudissement)",
                    "Installation interactive : Contr√¥le gestuel collectif d'√©l√©ments num√©riques"
                ],
                implementations: [
                    "Raspberry Pi 4 avec Azure Kinect SDK pour d√©tection de base",
                    "OpenPose ou MediaPipe pour estimation de pose robuste",
                    "Fusion multi-capteurs pour reconstruction 3D pr√©cise",
                    "Algorithmes de reconnaissance de gestes bas√©s sur s√©quences temporelles"
                ],
                interactiveShows: [
                    "Projection mapping suivant exactement les mouvements du public",
                    "G√©n√©ration musicale bas√©e sur la complexit√© gestuelle d√©tect√©e",
                    "Avatar num√©rique g√©ant repr√©sentant les gestes collectifs",
                    "Jeu interactif contr√¥l√© par coordination gestuelle de groupe"
                ],
                technicalSpecs: [
                    "Tracking de squelette : 25 articulations par personne, jusqu'√† 6 corps",
                    "Pr√©cision de profondeur : ¬±2mm √† distance 1m",
                    "Fr√©quence d'images : 30fps synchronis√© entre cam√©ras",
                    "Champ de vue : 120¬∞√ó120¬∞ avec optimisation de recouvrement",
                    "Port√©e : 0.5-5.5m distance de tracking efficace"
                ],
                limitations: [
                    "Tracking limit√© dans foules tr√®s denses (>4 personnes/m¬≤)",
                    "N√©cessite ligne de vue directe vers sujets",
                    "Performance d√©grad√©e avec conditions d'√©clairage extr√™mes"
                ],
                references: [
                    "https://docs.microsoft.com/en-us/azure/kinect-dk/",
                    "https://github.com/microsoft/Azure-Kinect-Samples",
                    "https://doi.org/10.1109/CVPR.2019.00584",
                    "https://www.microsoft.com/en-us/research/project/kinect-for-windows/"
                ]
            },
            {
                title: "Interface de Vote √âmotionnel en Temps R√©el",
                description: "Syst√®me d'interfaces tactiles permettant au public d'exprimer ses √©motions en temps r√©el pendant un spectacle, cr√©ant une boucle de feedback imm√©diate entre audience et performance.",
                dataTypes: ["social", "comportemental"],
                privacy: "faible",
                participation: "actif",
                implementation: "interactif",
                technology: "appareils-mobiles",
                showApplications: ["temps-reel", "interactif-public", "adaptatif"],
                hardware: [
                    "Applications web progressives (PWA) multi-plateformes",
                    "Tablettes tactiles distribu√©es dans l'audience",
                    "Bo√Ætiers de vote physiques avec boutons color√©s",
                    "√âcrans interactifs muraux pour vote collectif"
                ],
                scenarios: [
                    "Concert interactif : Le public influence la setlist en temps r√©el",
                    "Spectacle immersif : L'audience vote pour diriger le d√©roulement narratif",
                    "Performance de jazz : Les musiciens adaptent leur improvisation aux r√©actions",
                    "Th√©√¢tre exp√©rimental : Les √©motions du public modifient l'√©clairage et la sc√©nographie"
                ],
                implementations: [
                    "Raspberry Pi comme serveurs de vote avec interfaces web simples",
                    "WebSocket pour transmission temps r√©el des votes √©motionnels",
                    "Visualisation en direct des tendances √©motionnelles collectives",
                    "APIs de contr√¥le pour √©quipements de spectacle (DMX, OSC, MIDI)"
                ],
                interactiveShows: [
                    "√âclairage qui refl√®te instantan√©ment l'humeur collective vot√©e",
                    "S√©lection musicale adaptative bas√©e sur pr√©f√©rences exprim√©es",
                    "Effets visuels modul√©s par l'intensit√© √©motionnelle mesur√©e",
                    "Personnages virtuels r√©agissant aux √©motions du public"
                ],
                technicalSpecs: [
                    "√âchelles valence/arousal : -5 √† +5 r√©solution √©motionnelle",
                    "Latence de vote : <100ms de l'interaction √† l'affichage",
                    "Capacit√© : 1000+ participants simultan√©s",
                    "Types d'√©motions : 8 √©motions de base + intensit√©"
                ],
                limitations: [
                    "Participation volontaire uniquement",
                    "Biais d'auto-s√©lection des participants",
                    "Fatigue de participation sur longue dur√©e"
                ],
                references: [
                    "https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps",
                    "https://socket.io/docs/v4/",
                    "https://www.openlighting.org/ola/",
                    "https://opensoundcontrol.stanford.edu/"
                ]
            },
            {
                title: "Analyse de Flux de Foule par Smartphone",
                description: "Application mobile distribu√©e utilisant GPS, acc√©l√©rom√®tres et capteurs de proximit√© pour cr√©er une carte collaborative des mouvements de foule et patterns de circulation.",
                dataTypes: ["spatial", "comportemental"],
                privacy: "moyenne",
                participation: "actif",
                implementation: "interactif",
                technology: "appareils-mobiles",
                showApplications: ["temps-reel", "visualisation"],
                hardware: [
                    "GPS int√©gr√© avec pr√©cision 3-5m",
                    "Acc√©l√©rom√®tres et gyroscopes pour d√©tection de mouvement",
                    "Capteurs de proximit√© Bluetooth/WiFi",
                    "Boussole magn√©tique pour orientation"
                ],
                scenarios: [
                    "Grand festival : Optimisation des flux entre sc√®nes en temps r√©el",
                    "Manifestation : Monitoring de s√©curit√© et pr√©vention d'embouteillages",
                    "Marathon : Analyse des patterns de course et points de ralentissement",
                    "Centre commercial : √âtudes de comportement d'achat et de circulation"
                ],
                implementations: [
                    "Application React Native avec g√©olocalisation temps r√©el",
                    "Serveur Node.js avec base de donn√©es g√©ospatiale (PostGIS)",
                    "Algorithmes de clustering pour d√©tection de groupes mobiles",
                    "Cartes de chaleur temps r√©el avec Leaflet.js ou Mapbox"
                ],
                interactiveShows: [
                    "Guidage lumineux dynamique optimisant les flux de public",
                    "R√©v√©lation progressive d'installations selon patterns d'exploration",
                    "Musique ambiante qui √©volue selon densit√© et vitesse de circulation",
                    "Jeux de piste collectifs bas√©s sur g√©olocalisation collaborative"
                ],
                technicalSpecs: [
                    "Pr√©cision GPS : 3-5m en conditions normales, 1m avec correction",
                    "Fr√©quence de mise √† jour : 1-5Hz selon consommation batterie",
                    "D√©tection de proximit√© : 10-100m port√©e Bluetooth/WiFi",
                    "Autonomie : Optimis√©e pour 8+ heures d'usage continu"
                ],
                limitations: [
                    "Consommation batterie significative avec GPS continu",
                    "Pr√©cision d√©grad√©e en int√©rieur ou zones urbaines denses",
                    "N√©cessite participation active et installation d'application"
                ],
                references: [
                    "https://reactnative.dev/docs/geolocation",
                    "https://postgis.net/",
                    "https://leafletjs.com/",
                    "https://docs.mapbox.com/help/tutorials/"
                ]
            },
            {
                title: "R√©seaux de Capteurs Environnementaux Collaboratifs",
                description: "Exploitation des capteurs environnementaux des smartphones (barom√®tre, thermom√®tre, capteur de lumi√®re) pour cr√©er un r√©seau de surveillance environnementale distribu√© √† travers la foule.",
                dataTypes: ["environnemental", "spatial"],
                privacy: "faible",
                participation: "actif",
                implementation: "interactif",
                technology: "appareils-mobiles",
                showApplications: ["temps-reel", "visualisation"],
                hardware: [
                    "Capteurs de pression barom√©trique smartphones (¬±1 hPa)",
                    "Capteurs de lumi√®re ambiante (mesure LUX)",
                    "Capteurs de temp√©rature (si disponibles)",
                    "Magn√©tom√®tre pour positionnement indoor"
                ],
                scenarios: [
                    "Festival en plein air : Cartographie microclimatique en temps r√©el",
                    "Concert en salle : Surveillance qualit√© de l'air et confort thermique",
                    "√âv√©nement sportif : Monitoring des conditions m√©t√©o localis√©es",
                    "Installation artistique : Cr√©ation d'≈ìuvres r√©actives aux conditions ambiantes"
                ],
                implementations: [
                    "Raspberry Pi comme hubs de collecte avec capteurs BME280",
                    "Base de donn√©es time-series (InfluxDB) pour donn√©es environnementales",
                    "Dashboard Grafana pour visualisation temps r√©el des conditions",
                    "API de pr√©diction m√©t√©o bas√©e sur donn√©es collaboratives"
                ],
                interactiveShows: [
                    "√âclairage adaptatif compensant les variations de luminosit√© naturelle",
                    "Effets visuels modul√©s par pression atmosph√©rique (sensation d'altitude)",
                    "Climatisation interactive r√©agissant aux mesures de temp√©rature collective",
                    "Compositions sonores √©voluant avec les conditions m√©t√©orologiques"
                ],
                technicalSpecs: [
                    "Pr√©cision pression : ¬±1 hPa (¬±8m altitude)",
                    "Plage capteur de lumi√®re : 0.1-100,000 LUX",
                    "Fr√©quence de mise √† jour : 1-10Hz par appareil",
                    "R√©solution spatiale : 1-5m selon densit√© de foule"
                ],
                limitations: [
                    "Qualit√© capteurs varie significativement entre appareils",
                    "Affect√© par placement de l'appareil (poche, main, sac)",
                    "N√©cessite participation active utilisateur et usage batterie"
                ],
                references: [
                    "https://developer.android.com/guide/topics/sensors/sensors_environment",
                    "https://developer.apple.com/documentation/coremotion/cmaltimeter",
                    "https://doi.org/10.1145/3241539.3241572",
                    "https://grafana.com/docs/"
                ]
            },
            {
                title: "D√©tection Infrasonore G√©n√©r√©e par la Foule",
                description: "D√©tection et analyse d'infrasons (en dessous de 20Hz) g√©n√©r√©s par les mouvements de foule large, fournissant alerte pr√©coce pour conditions dangereuses de foule et patterns de mouvement de masse.",
                dataTypes: ["acoustique", "comportemental", "environnemental"],
                privacy: "faible",
                participation: "aucune",
                implementation: "infrastructure",
                technology: "capteurs",
                showApplications: ["temps-reel"],
                hardware: [
                    "Microphones infrasonores (Chaparral Physics Model 5)",
                    "Acc√©l√©rom√®tres basse fr√©quence pour couplage au sol",
                    "Syst√®mes de r√©duction de bruit de vent",
                    "Syst√®mes ADC haute r√©solution 24-bit",
                    "Synchronisation GPS pour traitement de r√©seau"
                ],
                scenarios: [
                    "Grand stade : D√©tection pr√©coce de mouvements de foule dangereux",
                    "Festival massif : Monitoring des vagues humaines et synchronisation",
                    "√âv√©nement en plein air : Surveillance de la r√©ponse de foule aux catastrophes",
                    "Concert symphonique : D√©tection de l'immersion collective profonde"
                ],
                implementations: [
                    "Raspberry Pi avec ADC 24-bit pour capture infrasonore simple",
                    "Filtrage num√©rique pour isolation signaux 0.01-20Hz",
                    "Corr√©lation crois√©e pour localisation de sources infrasonores",
                    "Syst√®me d'alerte automatique pour patterns dangereux"
                ],
                interactiveShows: [
                    "Basses fr√©quences musicales synchronis√©es avec infrasons de foule",
                    "√âclairage ambiant r√©agissant aux vagues infrasonores d√©tect√©es",
                    "Effets haptiques transmis via infrastructure pour amplifier immersion",
                    "Cr√©ation d'instruments musicaux bas√©s sur r√©sonance collective"
                ],
                technicalSpecs: [
                    "Plage de fr√©quence : 0.01-20Hz avec r√©ponse plate",
                    "Gamme dynamique : 140dB minimum d√©tectable",
                    "R√©solution spatiale : 100m+ pour grands √©v√©nements de foule",
                    "Seuil de d√©tection : <0.1 Pa variations de pression"
                ],
                limitations: [
                    "Interf√©rences du vent, trafic et machines",
                    "N√©cessite foules importantes (1000+ personnes) pour signaux clairs",
                    "Pr√©cision d√©pendante des conditions m√©t√©o pour √©v√©nements ext√©rieurs"
                ],
                references: [
                    "https://www.chaparralphysics.com/",
                    "https://doi.org/10.1121/1.4976050",
                    "https://www.dtcc.edu/seismology/infrasound.html",
                    "https://www.ctbto.org/verification-regime/monitoring-technologies-how-they-work/infrasound-monitoring/"
                ]
            },
            {
                title: "Analyse Prox√©mique et Espace Personnel bas√©e Kinect",
                description: "Analyse des distances interpersonnelles, violations d'espace personnel et patterns de groupage social utilisant d√©tection de proximit√© bas√©e profondeur de multiples capteurs Kinect.",
                dataTypes: ["spatial", "comportemental", "social"],
                privacy: "elevee",
                participation: "aucune",
                implementation: "passif",
                technology: "vision-par-ordinateur",
                showApplications: ["visualisation", "adaptatif"],
                hardware: [
                    "Capteurs Kinect v2 avec couverture grand angle",
                    "Unit√©s de traitement fusion de profondeur",
                    "Mat√©riel d'inf√©rence machine learning",
                    "N≈ìuds de calcul edge pr√©servant la confidentialit√©"
                ],
                scenarios: [
                    "√âtude anthropologique : Analyse des normes culturelles d'espace personnel",
                    "Installation artistique : Visualisation des bulles sociales et territoires",
                    "√âv√©nement networking : Facilitation des interactions par proximit√©",
                    "Th√©√¢tre immersif : Adaptation de l'intimit√© selon confort spatial du public"
                ],
                implementations: [
                    "Raspberry Pi 4 avec Azure Kinect SDK pour d√©tection de base",
                    "Algorithmes de clustering DBSCAN pour d√©tection de groupes",
                    "Anonymisation temps r√©el par floutage de caract√©ristiques faciales",
                    "Visualisation de r√©seaux sociaux bas√©e sur proximit√© physique"
                ],
                interactiveShows: [
                    "√âclairage personnel cr√©ant des bulles lumineuses adapt√©es √† chaque individu",
                    "Sons spatialis√©s r√©agissant aux distances interpersonnelles",
                    "Interactions d√©clench√©es par rapprochement volontaire entre participants",
                    "Visualisation artistique des connexions sociales √©mergentes"
                ],
                technicalSpecs: [
                    "Pr√©cision mesure distance : ¬±5cm entre individus",
                    "D√©tection espace personnel : Analyse rayon 0.5-3m",
                    "Algorithme d√©tection groupe : Clustering DBSCAN",
                    "Tracking temporel : Historiques d'interaction 10 minutes"
                ],
                limitations: [
                    "Ne peut distinguer types de relations entre personnes",
                    "Pr√©cision diminue avec occlusions partielles",
                    "Biais culturels dans interpr√©tation espace personnel"
                ],
                references: [
                    "https://doi.org/10.1016/j.chb.2018.12.025",
                    "https://github.com/microsoft/kinect-2-libfreenect2",
                    "https://scikit-learn.org/stable/modules/clustering.html#dbscan",
                    "https://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html"
                ]
            },
            {
                title: "Interface de Contr√¥le Gestuel Collaboratif",
                description: "Syst√®me permettant au public de contr√¥ler collectivement des √©l√©ments du spectacle par gestes d√©tect√©s, cr√©ant une performance collaborative entre artistes et audience.",
                dataTypes: ["comportemental", "social"],
                privacy: "moyenne",
                participation: "actif",
                implementation: "interactif",
                technology: "vision-par-ordinateur",
                showApplications: ["temps-reel", "interactif-public", "adaptatif"],
                hardware: [
                    "Cam√©ras de profondeur distribu√©es (Azure Kinect, RealSense)",
                    "Capteurs de mouvement infrarouges",
                    "Processeurs edge pour reconnaissance gestuelle",
                    "Syst√®me de projection ou √©crans LED responsifs"
                ],
                scenarios: [
                    "Concert √©lectronique : Le public contr√¥le filtres et effets par gestes collectifs",
                    "Spectacle pour enfants : Participation gestuelle pour faire √©voluer l'histoire",
                    "Performance artistique : Co-cr√©ation en temps r√©el entre artiste et audience",
                    "Installation urbaine : Contr√¥le de l'√©clairage public par mouvements des passants"
                ],
                implementations: [
                    "Raspberry Pi avec cam√©ra PiCam et mod√®les TensorFlow Lite",
                    "Reconnaissance gestuelle avec MediaPipe Hands et Pose",
                    "Syst√®me de vote gestuel avec agr√©gation temps r√©el",
                    "Interface DMX/OSC pour contr√¥le √©quipements sc√©niques"
                ],
                interactiveShows: [
                    "√âclairage contr√¥l√© par mouvements de bras collectifs du public",
                    "G√©n√©ration musicale bas√©e sur complexit√© gestuelle d√©tect√©e",
                    "Projection mapping r√©agissant aux gestes directionnels",
                    "Personnages virtuels mimant et amplifiant les gestes du public"
                ],
                technicalSpecs: [
                    "Gestes reconnus : 10-20 gestes de base pr√©d√©finis",
                    "Latence reconnaissance : <200ms du geste √† la r√©action",
                    "Participants simultan√©s : 50-200 selon zone de couverture",
                    "Pr√©cision : 90%+ pour gestes simples en conditions optimales"
                ],
                limitations: [
                    "N√©cessite apprentissage des gestes par le public",
                    "Performance d√©grad√©e avec √©clairage inad√©quat",
                    "Confusion possible avec gestes involontaires"
                ],
                references: [
                    "https://mediapipe.dev/solutions/hands",
                    "https://tensorflow.org/lite/models/pose_estimation/overview",
                    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples",
                    "https://www.openlighting.org/ola/getting-started/"
                ]
            },
            {
                title: "Analyse de Nuages de Points 3D LiDAR",
                description: "Scanners LiDAR haute r√©solution cr√©ant des nuages de points 3D d√©taill√©s pour analyse pr√©cise de foule, tracking de mouvement et estimation de densit√© sans pr√©occupations de confidentialit√©.",
                dataTypes: ["spatial", "comportemental"],
                privacy: "faible",
                participation: "aucune",
                implementation: "passif",
                technology: "capteurs",
                showApplications: ["temps-reel", "visualisation"],
                hardware: [
                    "Velodyne VLP-32C Ultra Puck (port√©e 200m)",
                    "Ouster OS1-128 LiDAR solid-state",
                    "Livox Horizon LiDAR industriel",
                    "Clusters de calcul haute performance pour traitement nuages de points"
                ],
                scenarios: [
                    "Stade sportif : Analyse 3D des mouvements de vague dans les gradins",
                    "Place publique : Monitoring des flux pi√©tonniers et optimisation urbaine",
                    "Salon professionnel : Cartographie 3D des zones de congestion",
                    "Installation artistique : Cr√©ation de sculptures num√©riques bas√©es sur formes de foule"
                ],
                implementations: [
                    "Raspberry Pi 4 avec LiDAR 2D RPLiDAR A1M8 pour applications simples",
                    "Point Cloud Library (PCL) pour traitement et analyse",
                    "Algorithmes de clustering pour d√©tection de groupes humains",
                    "Visualisation temps r√©el avec Three.js et WebGL"
                ],
                interactiveShows: [
                    "Projection mapping 3D suivant exactement les contours de la foule",
                    "G√©n√©ration proc√©durale d'environnements 3D bas√©s sur positions du public",
                    "Sculpture lumineuse virtuelle moul√©e sur la forme collective",
                    "Effets particulaires interagissant physiquement avec l'espace occup√©"
                ],
                technicalSpecs: [
                    "Densit√© nuage de points : 2+ millions points/seconde",
                    "Pr√©cision de port√©e : ¬±2cm √† 100m",
                    "Champ de vue : 360¬∞ horizontal, 40¬∞ vertical",
                    "Traitement temps r√©el : 10-20Hz fr√©quence de mise √† jour"
                ],
                limitations: [
                    "Co√ªt √©lev√© des syst√®mes LiDAR industriels",
                    "Sensible aux conditions m√©t√©orologiques (pluie, brouillard)",
                    "Exigences massives de traitement de donn√©es"
                ],
                references: [
                    "https://velodynelidar.com/products/vlp-32c/",
                    "https://ouster.com/products/os1-lidar-sensor/",
                    "https://www.livoxtech.com/horizon",
                    "https://doi.org/10.1109/TITS.2020.2991573"
                ]
            },
            {
                title: "Reconnaissance d'Expressions Faciales et Analyse √âmotionnelle",
                description: "Syst√®mes avanc√©s de vision par ordinateur pour d√©tecter et analyser les expressions faciales afin de comprendre les √©tats √©motionnels collectifs et r√©actions individuelles dans les foules.",
                dataTypes: ["biometrique", "comportemental"],
                privacy: "critique",
                participation: "aucune",
                implementation: "passif",
                technology: "ia-ml",
                showApplications: ["temps-reel", "adaptatif"],
                hardware: [
                    "Cam√©ras haute r√©solution avec objectifs t√©l√©objectif",
                    "GPU NVIDIA RTX 4090 pour traitement temps r√©el",
                    "Cam√©ras Intel RealSense pour analyse faciale 3D",
                    "N≈ìuds de calcul edge pour traitement distribu√©"
                ],
                scenarios: [
                    "Spectacle humoristique : Mesure en temps r√©el de l'efficacit√© des blagues",
                    "Pr√©sentation TED : Analyse de l'engagement √©motionnel de l'audience",
                    "Concert classique : D√©tection des moments d'√©motion intense",
                    "Th√©√¢tre : Adaptation subtile de la performance selon r√©actions √©motionnelles"
                ],
                implementations: [
                    "Raspberry Pi 4 avec Google Coral TPU pour inf√©rence locale",
                    "Mod√®les CNN optimis√©s (MobileNet, EfficientNet) pour edge computing",
                    "Anonymisation temps r√©el par brouillage des caract√©ristiques faciales",
                    "API de classification √©motionnelle avec confidence scores"
                ],
                interactiveShows: [
                    "√âclairage ambiant qui refl√®te l'humeur collective dominante",
                    "Modulation musicale bas√©e sur l'intensit√© √©motionnelle mesur√©e",
                    "Feedback visuel amplifiant les √©motions positives d√©tect√©es",
                    "Adaptation narrative interactive selon √©tat √©motionnel du public"
                ],
                technicalSpecs: [
                    "Reconnaissance d'√©motions : 7 √©motions de base + neutre",
                    "Pr√©cision : 85-95% en conditions optimales",
                    "Vitesse de traitement : 30fps pour visages multiples",
                    "Distance de d√©tection : 2-20m selon cam√©ra"
                ],
                limitations: [
                    "Implications de confidentialit√© extr√™mement √©lev√©es",
                    "Biais culturels dans algorithmes de reconnaissance d'√©motions",
                    "Performance affect√©e par √©clairage et angles"
                ],
                references: [
                    "https://github.com/serengil/deepface",
                    "https://azure.microsoft.com/en-us/services/cognitive-services/face/",
                    "https://doi.org/10.1109/TAFFC.2019.2946540",
                    "https://mediapipe.dev/solutions/face_mesh"
                ]
            },
            {
                title: "Int√©gration d'Unit√©s de Mesure Inertielle (IMU) Portables",
                description: "Int√©gration avec trackers de fitness, montres connect√©es et appareils IMU d√©di√©s pour collecter des donn√©es pr√©cises de mouvement, orientation et activit√© de participants volontaires.",
                dataTypes: ["physiologique", "comportemental"],
                privacy: "elevee",
                participation: "dedique",
                implementation: "actif",
                technology: "appareils-mobiles",
                showApplications: ["temps-reel", "interactif-public", "adaptatif"],
                hardware: [
                    "Apple Watch Series 8/9 avec int√©gration HealthKit",
                    "Garmin Forerunner avec applications Connect IQ",
                    "Modules IMU personnalis√©s (BMI160, LSM6DSO)",
                    "Agr√©gation de donn√©es Bluetooth Low Energy"
                ],
                scenarios: [
                    "Cours de fitness collectif : Synchronisation pr√©cise des mouvements et encouragements",
                    "Festival de danse : Analyse de la complexit√© et intensit√© des mouvements",
                    "M√©ditation guid√©e : Monitoring de la stabilit√© et relaxation corporelle",
                    "Performance participative : Le public devient partie int√©grante du spectacle"
                ],
                implementations: [
                    "Raspberry Pi avec dongle Bluetooth pour agr√©gation de donn√©es IMU",
                    "Algorithmes de fusion de capteurs pour estimation de pose pr√©cise",
                    "Interface de contr√¥le gestuel pour participation directe au spectacle",
                    "Feedback haptique via montres connect√©es pour guidance"
                ],
                interactiveShows: [
                    "Orchestration musicale contr√¥l√©e par les mouvements collectifs du public",
                    "Chor√©graphie lumineuse qui amplifie et guide les gestes des participants",
                    "Cr√©ation collaborative d'≈ìuvres visuelles par le mouvement",
                    "Jeux interactifs √† grande √©chelle bas√©s sur coordination gestuelle"
                ],
                technicalSpecs: [
                    "Fr√©quence d'√©chantillonnage IMU : 50-200Hz",
                    "Pr√©cision fr√©quence cardiaque : ¬±2 BPM pendant activit√©",
                    "Autonomie batterie : 18+ heures surveillance continue",
                    "Transmission donn√©es : Temps r√©el via BLE ou WiFi"
                ],
                limitations: [
                    "N√©cessite possession d'appareils compatibles par utilisateurs",
                    "Pr√©occupations de confidentialit√© avec collecte de donn√©es de sant√©",
                    "Limit√© aux participants volontaires pour partage de donn√©es"
                ],
                references: [
                    "https://developer.apple.com/documentation/healthkit",
                    "https://developer.garmin.com/connect-iq/",
                    "https://www.bosch-sensortec.com/products/motion-sensors/imus/bmi160/",
                    "https://doi.org/10.1109/JBHI.2020.2988269"
                ]
            }
        ];

        // PARTIE 4 - Fonctions JavaScript et fermeture du document

        function renderMethods() {
            const grid = document.getElementById('methodsGrid');
            grid.innerHTML = '';

            filteredMethods.forEach(method => {
                const card = document.createElement('div');
                card.className = 'method-card';

                const privacyClass = `privacy-${method.privacy}`;
                const participationClass = `participation-${method.participation}`;

                card.innerHTML = `
                    <div class="method-title">${method.title}</div>
                    <div class="method-description">${method.description}</div>

                    <div class="method-details">
                        <div class="detail-section">
                            <div class="detail-title">Mat√©riel & √âquipement</div>
                            <div class="detail-content">
                                <ul class="detail-list">
                                    ${method.hardware.map(item => `<li>${item}</li>`).join('')}
                                </ul>
                            </div>
                        </div>

                        ${method.scenarios ? `
                        <div class="scenario-section">
                            <div class="scenario-title">Sc√©narios d'Usage</div>
                            <div class="scenario-content">
                                <ul class="detail-list">
                                    ${method.scenarios.map(item => `<li>${item}</li>`).join('')}
                                </ul>
                            </div>
                        </div>
                        ` : ''}

                        ${method.implementations ? `
                        <div class="implementation-section">
                            <div class="implementation-title">Impl√©mentations Techniques</div>
                            <div class="implementation-content">
                                <ul class="detail-list">
                                    ${method.implementations.map(item => `<li>${item}</li>`).join('')}
                                </ul>
                            </div>
                        </div>
                        ` : ''}

                        ${method.interactiveShows ? `
                        <div class="detail-section">
                            <div class="detail-title">Applications Spectacle Interactif</div>
                            <div class="detail-content">
                                <ul class="detail-list">
                                    ${method.interactiveShows.map(item => `<li>${item}</li>`).join('')}
                                </ul>
                            </div>
                        </div>
                        ` : ''}

                        <div class="detail-section">
                            <div class="detail-title">Sp√©cifications Techniques</div>
                            <div class="detail-content">
                                <ul class="detail-list">
                                    ${method.technicalSpecs.map(item => `<li>${item}</li>`).join('')}
                                </ul>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Limitations</div>
                            <div class="detail-content">
                                <ul class="detail-list">
                                    ${method.limitations.map(item => `<li>${item}</li>`).join('')}
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="references">
                        <h4>R√©f√©rences & Documentation</h4>
                        ${method.references.map(ref => `<a href="${ref}" target="_blank" class="reference-link">${ref}</a>`).join('')}
                    </div>

                    <div class="tags-container">
                        ${method.dataTypes.map(type => `<span class="tag">${type}</span>`).join('')}
                        <span class="tag ${privacyClass}">Confidentialit√©: ${method.privacy}</span>
                        <span class="tag ${participationClass}">Participation: ${method.participation}</span>
                        <span class="tag">${method.technology}</span>
                        <span class="tag">${method.implementation}</span>
                        ${method.showApplications ? method.showApplications.map(app => `<span class="tag">${app}</span>`).join('') : ''}
                    </div>
                `;

                grid.appendChild(card);
            });

            updateStats();
        }

        function updateStats() {
            document.getElementById('visibleCount').textContent = filteredMethods.length;
            document.getElementById('totalCount').textContent = methods.length;
        }

        function filterMethods() {
            const searchTerm = document.getElementById('searchInput').value.toLowerCase();

            filteredMethods = methods.filter(method => {
                // Search filter
                const matchesSearch = !searchTerm ||
                    method.title.toLowerCase().includes(searchTerm) ||
                    method.description.toLowerCase().includes(searchTerm) ||
                    method.dataTypes.some(type => type.includes(searchTerm)) ||
                    method.hardware.some(hw => hw.toLowerCase().includes(searchTerm)) ||
                    (method.scenarios && method.scenarios.some(scenario => scenario.toLowerCase().includes(searchTerm))) ||
                    (method.implementations && method.implementations.some(impl => impl.toLowerCase().includes(searchTerm))) ||
                    (method.interactiveShows && method.interactiveShows.some(show => show.toLowerCase().includes(searchTerm))) ||
                    method.technicalSpecs.some(spec => spec.toLowerCase().includes(searchTerm));

                // Category filters
                const matchesDataType = activeFilters.dataType.length === 0 ||
                    activeFilters.dataType.some(filter => method.dataTypes.includes(filter));

                const matchesPrivacy = activeFilters.privacy.length === 0 ||
                    activeFilters.privacy.includes(method.privacy);

                const matchesParticipation = activeFilters.participation.length === 0 ||
                    activeFilters.participation.includes(method.participation);

                const matchesTechnology = activeFilters.technology.length === 0 ||
                    activeFilters.technology.includes(method.technology);

                const matchesImplementation = activeFilters.implementation.length === 0 ||
                    activeFilters.implementation.includes(method.implementation);

                const matchesShow = activeFilters.show.length === 0 ||
                    (method.showApplications && activeFilters.show.some(filter => method.showApplications.includes(filter)));

                return matchesSearch && matchesDataType && matchesPrivacy &&
                       matchesParticipation && matchesTechnology && matchesImplementation && matchesShow;
            });

            renderMethods();
        }

        function clearFilters() {
            // Clear search
            document.getElementById('searchInput').value = '';

            // Clear all active filters
            activeFilters = {
                dataType: [],
                privacy: [],
                participation: [],
                technology: [],
                implementation: [],
                show: []
            };

            // Remove active classes
            document.querySelectorAll('.filter-tag.active').forEach(tag => {
                tag.classList.remove('active');
            });

            filterMethods();
        }

        // Event listeners
        document.getElementById('searchInput').addEventListener('input', filterMethods);

        // Filter tag click handlers
        document.querySelectorAll('.filter-tag').forEach(tag => {
            tag.addEventListener('click', function() {
                const filterValue = this.dataset.filter;
                const filterGroup = this.parentElement.parentElement;
                const groupId = filterGroup.querySelector('h3').textContent.toLowerCase()
                    .replace(' ', '').replace('required', '').replace('√©', 'e').replace('√®', 'e');

                let filterKey;
                switch(groupId) {
                    case 'typededonnees': filterKey = 'dataType'; break;
                    case 'niveaudeconfidentialite': filterKey = 'privacy'; break;
                    case 'participationrequise': filterKey = 'participation'; break;
                    case 'typedetechnologie': filterKey = 'technology'; break;
                    case 'implementation': filterKey = 'implementation'; break;
                    case 'applicationsspectacle': filterKey = 'show'; break;
                }

                this.classList.toggle('active');

                if (this.classList.contains('active')) {
                    if (!activeFilters[filterKey].includes(filterValue)) {
                        activeFilters[filterKey].push(filterValue);
                    }
                } else {
                    activeFilters[filterKey] = activeFilters[filterKey].filter(f => f !== filterValue);
                }

                filterMethods();
            });
        });

        // Initial render
        renderMethods();
    </script>
</body>
</html>
